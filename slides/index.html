<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>üçí Cherry Lip Sync</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/night.css">
		<link rel="stylesheet" type="text/css" href="./asciinema-player.css" />

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
    </head>
    <style>

        .reveal img {
            margin: 0;
            vertical-align: middle;
        }
        .reveal .leftalign {
            text-align: left;
            margin-left: 120px;
        }
        .blocktext {
            text-align: left;
            margin-left: auto;
            margin-right: auto;
            width: 12em;
        }
        .barlabel {
            display: inline-block;
            width: 179px;
            font-size: 32px;
            text-align: right;
        }
        .bar {
            display: inline-block;
            font-size: 24px;
            color: #ddd;
            background-color: #080;
        }
        .box {
            outline: solid 4px;
            border-radius: 100px;
            padding-bottom: 10px;
            padding-right: 40px;
            outline-color: #ff0;
        }
        .underline {
            border-bottom: 3px solid #ff0;
        }

        .reveal .slides section .fragment.step-fade-in-then-out {
        	opacity: 0;
        	display: none;
        }
        .reveal .slides section .fragment.step-fade-in-then-out.current-fragment {
        	opacity: 1;
    	    display: inline;
        }

        blockquote {
            text-align: left;
        }
        figcaption {
            font-size: 0.7em;
        }
        .container{
            display: flex;
        }
        .col {
            flex: 1;
        }
        .col2 { 
            flex: 2;
        }
        .data {
            color: #fe8;
        }
        .time {
            color: #8ef;
        }
        .cast {
            width: 100%;
            height: 400px;
        }
    </style>
	<body>
        <div class="reveal">
            <div class="slides">

                <section data-background-image="gfx/bg2.png">
                    <h2>üçí Cherry Lip Sync</h2>
                    <p>Nathan Whitehead</p>
                    <p>Boise Code Camp 2025</p>
                </section>
                <section data-markdown data-separator-notes="^Note:">

<textarea data-template>
<p class="leftalign">
    <a href="https://github.com/nwhitehead/cherry-lip-sync">
        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24"><path fill="currentColor" d="M12 2A10 10 0 0 0 2 12c0 4.42 2.87 8.17 6.84 9.5c.5.08.66-.23.66-.5v-1.69c-2.77.6-3.36-1.34-3.36-1.34c-.46-1.16-1.11-1.47-1.11-1.47c-.91-.62.07-.6.07-.6c1 .07 1.53 1.03 1.53 1.03c.87 1.52 2.34 1.07 2.91.83c.09-.65.35-1.09.63-1.34c-2.22-.25-4.55-1.11-4.55-4.92c0-1.11.38-2 1.03-2.71c-.1-.25-.45-1.29.1-2.64c0 0 .84-.27 2.75 1.02c.79-.22 1.65-.33 2.5-.33s1.71.11 2.5.33c1.91-1.29 2.75-1.02 2.75-1.02c.55 1.35.2 2.39.1 2.64c.65.71 1.03 1.6 1.03 2.71c0 3.82-2.34 4.66-4.57 4.91c.36.31.69.92.69 1.85V21c0 .27.16.59.67.5C19.14 20.16 22 16.42 22 12A10 10 0 0 0 12 2"/></svg>
        nwhitehead/cherry-lip-sync
    </a><br />
    <a href="https://bsky.app/profile/nathanwhitehead.bsky.social">
        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24"><path fill="currentColor" d="M12 11.388c-.906-1.761-3.372-5.044-5.665-6.662c-2.197-1.55-3.034-1.283-3.583-1.033C2.116 3.978 2 4.955 2 5.528c0 .575.315 4.709.52 5.4c.68 2.28 3.094 3.05 5.32 2.803c-3.26.483-6.157 1.67-2.36 5.898c4.178 4.325 5.726-.927 6.52-3.59c.794 2.663 1.708 7.726 6.444 3.59c3.556-3.59.977-5.415-2.283-5.898c2.225.247 4.64-.523 5.319-2.803c.205-.69.52-4.825.52-5.399c0-.575-.116-1.55-.752-1.838c-.549-.248-1.386-.517-3.583 1.033c-2.293 1.621-4.76 4.904-5.665 6.664"/></svg>
        nathanwhitehead.bsky.social
    </a><br />
    <a href="mailto:nwhitehe@gmail.com">
        <svg xmlns="http://www.w3.org/2000/svg" width="48" height="48" viewBox="0 0 24 24"><path fill="currentColor" d="M20 4H4c-1.1 0-1.99.9-1.99 2L2 18c0 1.1.9 2 2 2h16c1.1 0 2-.9 2-2V6c0-1.1-.9-2-2-2m0 4l-8 5l-8-5V6l8 5l8-5z"/></svg>
        nwhitehe@gmail.com
    </a><br />
    <br />
    Slides at
    <a href="https://nwhitehead.github.io/slides/">
        nwhitehead.github.io/slides/
    </a><br />
</p>

<!-- ---
## Agenda

1. Why lip sync?
2. Data where?
3. Model how?
4. Deployment
5. Final thoughts -->

---
## Why lip sync?

Note:
These are speaker notes.
---
Cinema  
100 years of talking characters

<p class="leftalign">
    <img width="180" src="gfx/kentucky.png" /> <em>My Old Kentucky Home</em> (1926) <br />
    <img width="180" src="gfx/snowwhite.png" /> <em>Snow White</em> (1937) <br />
    <img width="180" src="gfx/chihiro.png" /> <em>Spirited Away</em> (2001) <br />
    <img width="180" src="gfx/transformer.png" /> <em>Transformers One</em> (2024) <br />
</p>

---
Video Games  
30 years of talking characters
<p class="leftalign">
    <img width="180" src="gfx/fullthrottle.jpg" /> <em>Full Throttle</em> (1995) <br />
    <img width="180" src="gfx/alyx.jpg" /> <em>Half Life 2</em> (2005) <br />
    <img width="180" src="gfx/mafia2.jpg" /> <em>Mafia 2</em> (2010) <br />
    <img width="180" src="gfx/last.png" /> <em>The Last of Us: Part II</em> (2020) <br />
</p>
---
VTubers  
10 years of digital puppetry
<p class="leftalign">
    <img width="130" src="gfx/kizunaai.png" /> <em>Kizuna AI</em> (2016-) <br />
    <img width="180" src="gfx/opera.png" /> <em>Ironmouse</em> (2017-) <br />
    <img width="180" src="gfx/gawr.png" /> <em>Gawr Gura</em> (2020-) <br />
    <img width="150" src="gfx/kuzuha.png" /> <em>Kuzuha</em> (2020-) <br />
</p>
---
What do kids want to be when they grow up?
(_Japan_)
<p class="leftalign" style="transform: scale(0.8);">
    <span class="barlabel">Teacher</span> <span style="width: 650px;" class="bar">6.5%</span><br />
    <span class="barlabel">Illustrator</span> <span style="width: 580px;" class="bar">5.8%</span><br />
    <span class="barlabel">Singer</span> <span style="width: 520px;" class="bar">5.2%</span><br />
    <span class="box"><span class="barlabel">VTuber</span> <span style="width: 460px;" class="bar">4.6%</span></span><br />
    <span class="barlabel">Actor</span> <span style="width: 430px;" class="bar">4.3%</span><br />
    <span class="barlabel">YouTuber</span> <span style="width: 350px;" class="bar">3.5%</span><br />
    <span class="barlabel">Doctor</span> <span style="width: 350px;" class="bar">3.5%</span><br />
    <span class="barlabel">Idol</span> <span style="width: 350px;" class="bar">3.5%</span><br />
    <span class="barlabel">Musician</span> <span style="width: 340px;" class="bar">3.4%</span><br />
    <span class="barlabel">Civil...</span> <span style="width: 320px;" class="bar">3.2%</span><br />
</p>
<small><a href="https://kids.nifty.com/research/work_20250102/">https://kids.nifty.com/research/work_20250102/</a></small>
---
What do kids want to be when they grow up?
(_USA_)
<p class="leftalign" style="transform: scale(0.8);">
    <span class="box"><span class="barlabel">V/YouTuber</span> <span style="width: calc(29px*20);" class="bar">29%</span></span><br />
    <span class="barlabel">Teacher</span> <span style="width: calc(26px*20);" class="bar">26%</span><br />
    <span class="barlabel">Athlete</span> <span style="width: calc(23px*20);" class="bar">23%</span><br />
    <span class="barlabel">Musician</span> <span style="width: calc(19px*20);" class="bar">19%</span><br />
    <span class="barlabel">Astronaut</span> <span style="width: calc(11px*20);" class="bar">11%</span><br />
</p>
<small><a href="https://www.prnewswire.com/news-releases/lego-group-kicks-off-global-program-to-inspire-the-next-generation-of-space-explorers-as-nasa-celebrates-50-years-of-moon-landing-300885423.html">https://www.prnewswire.com/news-releases/lego-group-...-300885423.html</a></small>
---

> allowing yourself to speak through a character not only makes it more comfortable but also helps you find your voice as well
>
> watching your own character babble to your own words in real time is always fun
>
> ‚Äî <cite>luna olmewe (creator of Veadotube)</cite>
---

### Veadotube

<img src="gfx/veadotube.webp" />

<small><a href="https://veado.tube/">https://veado.tube/</a></small>

---
### 2D Lip Sync

<p class="leftalign">
    Input
    <!-- <img src="gfx/waveform.png" class="fragment step-fade-in-then-out" /> -->
    <img src="gfx/output.png" />
    <p style="margin-bottom: 200px;" />
</p>
---
### Output

<img src="gfx/tsv.png" />
<img src="gfx/Chart2.png" width="400px" />

2D lip sync uses 4 to 16 visemes.
---
## Data where?
* Audio
* Annotations
---
### Audio

LibriSpeech
* 1000 hours of reading public domain books
* Mostly English
* 2500 unique speakers
* Gender balanced
* Transcribed, segmented, filtered

<small><a href="https://www.openslr.org/12">https://www.openslr.org/12</a> (CC-BY) üò∫</small>
---
### Rhubarb lip sync

<figure>
<img src="gfx/thimbleweed.png" width="500px"/>
<figcaption>Thimbleweed Park (2014)</figcaption>
</figure>

* audio ‚Üí phonemes (pre deep learning)
* phonemes ‚Üí visemes (fixed algorithm)
<small><a href="https://github.com/DanielSWolf/rhubarb-lip-sync">https://github.com/DanielSWolf/rhubarb-lip-sync</a> (MIT BSD) üò∫</small>
---
### Oculus Lip Sync
<img src="gfx/lips.gif" />

* audio ‚Üí viseme weights
* _Unity_ and _Unreal Engine_ plugins

<small><a href="https://developers.meta.com/horizon/documentation/native/audio-ovrlipsync-native">https://developers.meta.com/horizon/documentation/native/audio-ovrlipsync-native</a></small>

<small>(SDK license, closed source model) üòêÔ∏è</small>
---
### Adobe Animate
<img src="gfx/adobe_animate.webp" />

* audio ‚Üí keyframes

<small><a href="https://arxiv.org/abs/1910.08685">D. Aneja, W. Li. Real-Time Lip Sync for Live 2D Animation (2019)</a></small>

<small>(Patented proprietary model) üò≠</small>

---
### MeloTTS

<div class="container">
    <div class="col">
        <img src="gfx/vits_infer.png" />
    </div>
    <div class="col">
<ul>
<li>text ‚Üí phonemes ‚Üí timing ‚Üí audio</li>
<li>phonemes ‚Üí visemes (fixed algorithm)</li>
</ul>
    </div>
</div>
<small><a href="https://github.com/myshell-ai/MeloTTS">https://github.com/myshell-ai/MeloTTS</a> (MIT) üò∫</small>
---
### Phoneme Map code

```python
visemes = { 'X', 'A', 'B', 'C', 'D', 'E', 'F', 'G',
            'H', 'I', 'J', 'K' }

# Map goes from phoneme to list of visemes.
p2v_map = {
    'AH': ['D'],
    'AO': ['D'],
    'AW': ['D', 'F'],
    'AY': ['D', 'I'],
    'B': ['A'],
    ...
}
```
---
```python
for i, (ph, t) in list(enumerate(zip(phone_list, timestamps))):
    if ph in p2v_map:
        v = p2v_map[ph]
    else:
        v = ['sil']
    visemes.extend(v)
    end_time = timestamps[i + 1]
    n = len(v)
    # Evenly space timing if more than 1 viseme
    vtimestamps.extend([
        round(t + i / n * (end_time - t))
        for i in range(n)
    ])
```
---
### Audio to Vectors

<div class="container">
    <div class="col">
        <img src="gfx/stft_output.png" width="600px"/>
    </div>
    <div class="col">
        <p class="blocktext">
            <span>Resample to 16 kHz</span><br />
            <span>Window length 25 ms</span><br />
            <span>Hann window</span><br />
            <span>Hop length 10 ms</span><br />
            <span>Group FFT into 13 bins</span><br />
            <span>100 vectors per second</span><br />
            <small>Research into voice audio goes back to invention of telephone, all this stuff is just package defaults in <code>torchaudio</code>.</small>
        </p>
    </div>
</div>
<p>
    <small>Diagram from <a href="https://www.mathworks.com/help/dsp/ref/dsp.stft.html">https://www.mathworks.com/help/dsp/ref/dsp.stft.html</a></small>
</p>
---
### Building Dataset

|          | Data | Creation time |
| -------- | ---- | ------------- |
| Manual annotation | <span class="data">30 sec</span> | <span class="time">8 hours</span> |
| MeloTTS tweaks | <span class="data">10 min</span> | <span class="time">8 hours</span> |
| Tool output review | <span class="data">60 min</span> | <span class="time">8 hours</span> |

Total about 1 hour of training data. (235 MB)
---
```python
import torchaudio
import pandas as pd
# ...
samples, rate = torchaudio.load('./data/audio-600.mp3')
samples = samples.numpy()[0] # Take left channel samples
# ...
entries = []
visemes = []
# ... Parse XML file output in loop
    visemes.append(viseme)
# ... Fixup visemes to be at 100 Hz
entries.append({ 'audio': samples, 'visemes': visemes })
# ...
data = pd.DataFrame(entries)
data.to_parquet('./data/lipsync.parquet')
```
<small><a href="https://pandas.pydata.org/">https://pandas.pydata.org/</a></small>
---
## How lip sync?

Let's build up the full model in PyTorch

and speedrun the history of deep learning.
---
### Deep Learning

<img src="gfx/UDL.jpg" width="400" />

<small><a href="https://udlbook.github.io/udlbook/">https://udlbook.github.io/udlbook/</a> (free) üò∫</small>
---
### Supervised Learning

<img src="gfx/youarehere.svg" width="600" />

* Model maps input ‚Üí output
* Universal function approximation
---
### Smallest Model - Fit a Line

* One number input
* One number output

<small>(Gauss, 1800)</small>
---
<video data-autoplay src="gfx/SGD.mp4" poster="gfx/SGD.png"></video>
---
<video data-autoplay src="gfx/Loss.mp4" poster="gfx/Loss.png"></video>
---
### PyTorch Code

```python
class SimpleModel(torch.nn.Module): 
    def __init__(self): 
        super().__init__()
        # Model has single linear layer
        # 1 scalar input, 1 scalar output
        self.layer = torch.nn.Linear(1, 1)

    def forward(self, x):
        return self.layer(x)
```
---
```python
# Instantiate our model
model = SimpleModel()
# Define loss function
criterion = torch.nn.MSELoss()       # Mean-squared error
# Create SGD-type optimizer
optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

# Training loop
for epoch in range(3000):
    optimizer.zero_grad()            # Clear gradients
    pred_y = model(data_x)           # Get predictions
    loss = criterion(pred_y, data_y) # Compute loss
    loss.backward()                  # Compute gradient
    optimizer.step()                 # Update model parameters
```
---
### What is PyTorch doing for us?

<p class="leftalign">
    <span>üß± Providing blocks (e.g. <code>torch.nn.Linear</code>)</span><br />
    <span>ü§° Random model initialization</span><br />
    <span>üëú Batching, streaming data</span><br />
    <span>ü™Ñ Computing gradients automagically</span><br />
    <span>‚ö° Updating model</span><br />
    <span>üòé No math required</span><br />
</p>
---
### Artificial Neural Network

* Multiple inputs
* Multiple outputs
* Non-linear activation

<small>(Rosenblatt, 1950)</small>
---
<video data-autoplay src="gfx/LinearScene.mp4" poster="gfx/LinearScene.png"></video>
---
<video data-autoplay src="gfx/LinearScene2.mp4" poster="gfx/LinearScene2.png"></video>
---
<video data-autoplay src="gfx/MultiInput.mp4" poster="gfx/MultiInput.png"></video>
---
```python
class NeuralNetModel(torch.nn.Module): 
    def __init__(self): 
        super().__init__()
        # Input length 6, output length 5
        self.linear = torch.nn.Linear(6, 5)
        # Sigmoid activation
        self.activation = torch.nn.Sigmoid()

    def forward(self, x):
        return self.activation(self.linear(x))

```
---
### Multi-Layer Neural Network

* Stacked layers
* Linear / non-linear activations alternate

<small>(Hinton, 1986)</small>
---
<video data-autoplay src="gfx/MLP.mp4" poster="gfx/MLP.png"></video>
---
```python
class MLP(torch.nn.Module): 
    def __init__(self): 
        super().__init__()
        self.net = torch.nn.Sequential(
            torch.nn.Linear(784, 200),
            torch.nn.Sigmoid(),
            torch.nn.Linear(200, 200),
            torch.nn.Sigmoid(),
            torch.nn.Linear(200, 10),
            torch.nn.Sigmoid(),
        )

    def forward(self, x):
        return self.net(x)
---
### MNIST

* Digit recognition of 28x28 grayscale images
* MLP trains in a couple minutes on CPU
* Ignores spatial structure

<small><a href="https://en.wikipedia.org/wiki/MNIST_database">(MNIST 1994)</a></small>
---
### Deep Learning

<div class="container">
    <div class="col">
        <img src="gfx/xkcd_1838_machine_learning.png" />
        <small><a href="https://xkcd.com/1838/">https://xkcd.com/1838/</a></small>
    </div>
    <div class="col">
        <p class="blocktext">
            How do we use spatial structure? <br />
            How do we train deeper networks? <br />
            <br />
            <span class="underline">üïó How do we use time?</span>
        </p>
    </div>
</div>

---
### Time Series

<video data-autoplay src="gfx/TimeSeries.mp4" poster="gfx/TimeSeries.png"></video>
---
### GRU - Gated Recurrent Unit
* Explicit internal state
* Old state and current input ‚Üí candidate state
* Reset gate - How much to ignore old state?
* Update gate - How much to update state?

<small>(Cho et al., 2014)</small>
---
<video data-autoplay src="gfx/GRUScene.mp4" poster="gfx/GRUScene.png"></video>
---
<video data-autoplay src="gfx/GRUBoxScene.mp4" poster="gfx/GRUBoxScene.png"></video>
---
<video data-autoplay src="gfx/LipSyncGRU.mp4" poster="gfx/LipSyncGRU.png"></video>
---
```python
class SelectItem(nn.Module):
    # ...constructor takes 1 arg 'index'
    def forward(self, inputs):
        return inputs[self.index]

class LipSyncGRU(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            # batch_first means input is (Batch, Time, D_in)
            nn.GRU(26, 32, batch_first=True),
            SelectItem(0), # Take first output
            nn.Linear(32, 12))
    def forward(self, x):
        return self.net(x)
---
### Training LipSync (v1) CPU

<div class="cast" id="cpucast"></div>

<small><p>00:02:38 on AMD Ryzen 5 3600 (4.1 GHz)</p></small>
---
### Logging and Validation
```python
from torch.utils.tensorboard import SummaryWriter
writer = SummaryWriter()
# ... setup
for epoch in range(epochs):
    # ... normal training, use data_x and data_y (90%)
    # Validate, use validate_x and validate_y    (10%)
    with torch.no_grad(): # turn off learning
        pred_y = model(validate_x)
        validate_loss = criterion(pred_y, validate_y)
    # Log how we are doing
    writer.add_scalars('Loss', {
        'Train': training_loss,
        'Validate': validate_loss,
    }, epoch)
```
---
### TensorBoard

<img src="gfx/loss1.png"></img>
---
### Torch: GPU

```python
# Device configuration
device = torch.device('cuda' if torch.cuda.is_available()
                      else 'cpu')
# ...
# OLD
data_x = data['spectrogram']
data_y = data['visemes']

# NEW
data_x = data['spectrogram'].to(device)
data_y = data['visemes'].to(device)

```
---
### Training LipSync (v1) GPU

<div class="cast" id="gpucast"></div>

<small><p>00:01:01 on NVIDIA RTX 3090 FE</p></small>

---
### Tweak it

* Bigger hidden state?
* Stack more layers?
* More bins in spectrogram on input?

---
<video data-autoplay src="gfx/LipSyncGRU2.mp4" poster="gfx/LipSyncGRU2.png"></video>
---
### LipSync (v2)

```python
class LipSyncGRU(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(
            nn.GRU(26, 80, batch_first=True),
            SelectItem(0),
            nn.GRU(80, 80, batch_first=True),
            SelectItem(0),
            nn.Linear(80, 12),
        )

    def forward(self, x):
        return self.net(x)
```
---
### TensorBoard

<img src="gfx/loss2.png"></img>

Overfitting!

---
### Extra tricks

<img src="gfx/bench.png" />

* Dropout
    * Randomly cut out parts of data.
    * Practice with random players benched.
    * _Force team to not depend on star players._

```python
# Dropout layer with 20% coverage
# Put anywhere, fits any shape data
nn.Dropout(p=0.2)
```
---
### Extra Tricks 

<img src="gfx/vu.png" width="150px" />

* `BatchNorm`
    * Automatically scale input vectors.
    * Treats each channel independently.
    * _Like an audio compressor._

```python
# Input is (Batch, Time, D_in)
Permute((0, 2, 1)),
# BatchNorm1d needs (Batch, D_in, Time)
nn.BatchNorm1d(input_size),
# Go back to normal
Permute((0, 2, 1)),
```
---
### LipSync (v3)

```python
self.net = nn.Sequential(
    Permute((0, 2, 1)),
    nn.BatchNorm1d(26),
    Permute((0, 2, 1)),
    nn.Dropout(p=0.2),
    nn.GRU(26, 80, batch_first=True),
    SelectItem(0),
    nn.Dropout(p=0.2),
    nn.GRU(80, 80, batch_first=True),
    SelectItem(0),
    nn.Linear(80, 12),
)
```
---
### TensorBoard

<img src="gfx/loss3.png"></img>

---
### Demo

<video controls src="gfx/welcome.mp4" poster="gfx/welcome.png"></video>

---

## Deployment

---
### Saving and Loading

```python
# After training, save the model
torch.save(model.state_dict(), 'model.pt')
# ... in a different script
# Load the model
model = LipSyncGRU() # at this point it has random weights
modem.load_state_dict(
    torch.load('model.pt', weights_only=True,
        # Switch to using CPU after training on GPU
        map_location=torch.device('cpu'))
)
```
---
### Export to ONNX

```python
# ... Load model from weights
# Turn off training
model.eval()
# Send through one round of random input
x = torch.randn(1, max_time, feature_dims)
_ = model(x)
# Save as ONNX file
torch.onnx.export(model, x, 'model.onnx',
    export_params=True,
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={
        'input': { 1: 'time' },
        'output': { 1: 'time'}})
```

---
### ONNX Runtime (ORT)

<img src="gfx/onnx.png" width="200px" />

* Standard model interchange format
* All languages can use it, CPU, GPU
* Works in the browser with HW acceleration (!)

---

## Final Thoughts

---

### Model Tips

<p class="leftalign">
    <span>üß© Match input/output shapes</span><br />
    <span>‚õî Avoid NANs (but math is OK)</span><br />
    <span>üî™ Don't do in-place modification</span><br />
    <!-- <span>üå± Random numbers: consider seed flow</span><br /> -->
    <span><img src="gfx/bench.png" /> Use all the tricks (dropout, BatchNorm)</span><br />
</p>
---

### Deep Learning Plan

* Start with data
* Clean your data
* Compact your data
* Quick turnaround means progress (<10 min)
* Small models can be awesome

---
### Recommended tech

```python
uv          # Python environment management
python      # For training and model development
torch       # Deep learning framework
torchaudio  # Audio torch stuff
torchvision # Image and video torch stuff
pandas      # Data management in python
tensorboard # Viewing training progress graphs
tqdm        # Pretty progress bars
onnxruntime # Deploying and sharing models
```

---

<img src="gfx/ratatouille-34.webp" width="600"></img>

> Anyone can cook <span style="color: #dd4;">a machine learning model</span>... but only the fearless can be great.
>
> ‚Äî <cite>Gusteau, famous chef in **Ratatouille**</cite>

---
The End
---

### Slides

<ul>
    <li>Slides made with <a href="https://revealjs.com/">RevealJS</a>.</li>
    <li>Animations made with <a href="https://www.manim.community/">Manim</a>.</li>
    <li>Recordings made with <a href="https://asciinema.org/">asciinema</a>.</li>
</ul>
</textarea>












</section>
</div>
</div>
        <script src="./asciinema-player.min.js"></script>
        <script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			Reveal.initialize({
				hash: true,
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
        <script>
            (() => {
                addEventListener('keyup', (evt) => {
                    if (evt.key === '=') {
                        const slide = Reveal.getCurrentSlide();
                        const elems = slide.getElementsByTagName('video');
                        if (elems.length > 1) {
                            console.log('Too many videos');
                            return;
                        }
                        if (elems.length === 0) {
                            console.log('No video');
                            return;
                        }
                        const elem = elems[0];
                        if (elem.paused) {
                            elem.play();
                        } else {
                            elem.pause();
                        }
                    }
                });
            })();
        </script>
        <script>
            AsciinemaPlayer.create('./cpu2.cast', document.getElementById('cpucast'), {
                speed: 5,
                cols: 133,
                rows: 25,
                fit: 'both',
            });
            AsciinemaPlayer.create('./gpu.cast', document.getElementById('gpucast'), {
                speed: 5,
                cols: 133,
                rows: 25,
                fit: 'both',
            });
        </script>
	</body>
</html>
